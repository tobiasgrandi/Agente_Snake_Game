# 游냀 Agente Inteligente para Snake con REINFORCE

## Descripci칩n del Proyecto

Este proyecto consiste en un agente inteligente que aprende a jugar al cl치sico juego de la v칤bora (Snake) utilizando **aprendizaje por refuerzo** con el algoritmo **REINFORCE**, una t칠cnica basada en gradiente de pol칤tica.

El entorno del juego fue creado completamente desde cero, al igual que la implementaci칩n del algoritmo de aprendizaje. La red neuronal que representa la pol칤tica del agente fue desarrollada con **PyTorch**, y la visualizaci칩n del entorno se realiza con **Pygame** durante la evaluaci칩n.

## Funcionamiento

Durante el entrenamiento, el agente juega m칰ltiples partidas y aprende a mejorar su pol칤tica a partir de la se침al de recompensa que recibe. Esta se침al se define de la siguiente manera:

- **Comer la fruta:** recompensa positiva.

- **Colisionar con una pared o con s칤 mismo:** castigo (recompensa negativa).

- **Moverse sin colisionar:** peque침a recompensa neutra o ligeramente positiva.

- **Acercarse o alejarse de la fruta:** se entrega una recompensa proporcional a la distancia normalizada.

丘뙖잺 **Entrenamiento independiente del tama침o de cuadr칤cula:**  
El agente no depende de un tama침o espec칤fico del entorno. Gracias a la normalizaci칩n de las entradas y las recompensas (por ejemplo, la distancia a la fruta), puede entrenarse en una cuadr칤cula de tama침o `n x n` y luego aplicarse en una cuadr칤cula distinta `m x m` sin necesidad de reentrenar.

El algoritmo REINFORCE calcula el retorno acumulado con descuento y ajusta los pesos de la pol칤tica siguiendo la siguiente f칩rmula:

$$
\nabla J(\theta) = \mathbb{E}[\nabla \log \pi_\theta(a_t|s_t) \cdot G_t]
$$

donde $G_t$ es el retorno acumulado desde el paso $t$, calculado de forma recursiva.

## Evaluaci칩n del Aprendizaje

Para evaluar el desempe침o del agente, se registraron y visualizaron las siguientes m칠tricas:

- **Gr치fica de recompensa total por episodio** con promedio m칩vil.

- **Visualizaci칩n en tiempo real del agente jugando** mediante `pygame`, solo en la fase de evaluaci칩n.

Adem치s, se implement칩 una opci칩n para guardar el modelo entrenado y reutilizarlo luego sin necesidad de volver a entrenar.

## Resultados

El agente es capaz de mejorar progresivamente su desempe침o, logrando sobrevivir m치s tiempo, recolectar m치s frutas y evitar colisiones.

Se observa que a medida que el entrenamiento avanza:

- El puntaje promedio por episodio aumenta.
- La pol칤tica comienza a evitar movimientos aleatorios y a mantener trayectorias m치s eficientes.
- Se presenta una tendencia natural al zigzag debido a la estocasticidad de la pol칤tica.

> El comportamiento final depende fuertemente de los hiperpar치metros como la tasa de aprendizaje, `gamma` (factor de descuento), y la estructura de recompensas.

![Gr치fica de recompensas](plot.png)

## Instalaci칩n y Uso

Clona el repositorio:

```
git clone https://github.com/tobiasgrandi/Agente_Snake_Game
cd Agente_Snake_Game
```
Para visualizar la evaluaci칩n:
```
python evaluate.py
```
Para entrenar:
```
python train.py
```
